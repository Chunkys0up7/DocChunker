"""
ALL_IN_ONE_RAG_CHUNKER.py
------------------------
A single-file, top-down version of the RAG Document Preparation Toolkit for review and reference.
Sections:
- Imports & Setup
- File Readers (TXT, PDF, DOCX, PPTX)
- Chunker (Word-based)
- Metadata Extraction
- LLMConnector (Perplexity API, easily swappable)
- Streamlit UI (with LLM toggle)
"""
# === Imports & Setup ===
import streamlit as st
from pathlib import Path
import tempfile
import hashlib
from datetime import datetime
import os
import requests
import json
import re

# === File Readers ===
def read_txt(file_path):
    return Path(file_path).read_text(encoding='utf-8', errors='ignore')

def read_pdf(file_path):
    try:
        import PyPDF2
        with open(file_path, 'rb') as f:
            reader = PyPDF2.PdfReader(f)
            return '\n'.join(page.extract_text() or '' for page in reader.pages)
    except Exception as e:
        return ''

def read_docx(file_path):
    try:
        import docx
        doc = docx.Document(file_path)
        return '\n'.join([para.text for para in doc.paragraphs])
    except Exception as e:
        return ''

def read_pptx(file_path):
    try:
        import pptx
        prs = pptx.Presentation(file_path)
        text = []
        for slide in prs.slides:
            for shape in slide.shapes:
                if hasattr(shape, "text"):
                    text.append(shape.text)
        return '\n'.join(text)
    except Exception as e:
        return ''

def read_file_with_dispatcher(file_path):
    ext = str(file_path).lower()
    if ext.endswith('.txt'):
        return read_txt(file_path)
    elif ext.endswith('.pdf'):
        return read_pdf(file_path)
    elif ext.endswith('.docx'):
        return read_docx(file_path)
    elif ext.endswith('.pptx'):
        return read_pptx(file_path)
    return ''

# === Chunker ===
def chunk_text_by_words(text, chunk_size):
    words = text.split()
    chunks = []
    for i in range(0, len(words), chunk_size):
        chunk = ' '.join(words[i:i+chunk_size])
        if chunk:
            chunks.append(chunk)
    return chunks

# === Metadata Extraction ===
def extract_rich_metadata(file_path, chunk, chunk_number, total_chunks, full_text=None):
    stats = Path(file_path).stat()
    metadata = {
        'file_hash': hashlib.md5(Path(file_path).read_bytes()).hexdigest(),
        'original_size': stats.st_size,
        'created_date': datetime.fromtimestamp(stats.st_ctime).isoformat(),
        'modified_date': datetime.fromtimestamp(stats.st_mtime).isoformat(),
        'processing_time': datetime.now().isoformat(),
        'chunk_number': chunk_number,
        'total_chunks': total_chunks,
        'word_count': len(chunk.split()),
    }
    # Heading heuristic
    lines = [line.strip() for line in chunk.splitlines() if line.strip()]
    if lines:
        first_line = lines[0]
        if first_line.isupper() or re.match(r'^[A-Z][a-z]+( [A-Z][a-z]+)*$', first_line):
            metadata['section_heading'] = first_line
        metadata['first_line'] = lines[0]
        metadata['last_line'] = lines[-1]
    # Keyword heuristic
    words = re.findall(r'\b\w{5,}\b', chunk.lower())
    freq = {}
    for w in words:
        freq[w] = freq.get(w, 0) + 1
    sorted_words = sorted(freq, key=freq.get, reverse=True)
    if sorted_words:
        metadata['keywords'] = ', '.join(sorted_words[:5])
    return metadata

# === LLMConnector ===
class LLMConnector:
    def __init__(self, api_key, model="sonar", api_url="https://api.perplexity.ai/chat/completions"):
        self.api_key = api_key
        self.model = model
        self.api_url = api_url

    def truncate_text(self, text, max_chars=4000):
        return text[:max_chars]

    def clean_text(self, text):
        return re.sub(r'[^\x09\x0A\x0D\x20-\x7E]', '', text)

    def call_llm(self, prompt, max_tokens=256, system_prompt="Be precise and concise."):
        headers = {
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json"
        }
        data = {
            "model": self.model,
            "messages": [
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": prompt}
            ],
            "max_tokens": max_tokens,
            "temperature": 0.2
        }
        try:
            response = requests.post(self.api_url, headers=headers, json=data, timeout=30)
            response.raise_for_status()
            result = response.json()
            return result['choices'][0]['message']['content'].strip()
        except Exception as e:
            return f"[LLM error: {e}]"

    def enrich_metadata(self, chunk, full_text=None):
        chunk_trunc = self.truncate_text(chunk)
        chunk_clean = self.clean_text(chunk_trunc)
        summary_prompt = f"Summarize the following document chunk in 1-2 sentences:\n\n{chunk_clean}"
        keywords_prompt = f"Extract 5-10 keywords or topics from the following document chunk:\n\n{chunk_clean}"
        section_prompt = f"If this chunk is part of a chapter or section, provide the likely section or chapter title. If not, say 'None'.\n\n{chunk_clean}"
        summary = self.call_llm(summary_prompt, max_tokens=128)
        keywords = self.call_llm(keywords_prompt, max_tokens=64)
        section = self.call_llm(section_prompt, max_tokens=32)
        return {
            "llm_summary": summary,
            "llm_keywords": keywords,
            "llm_section": section
        }

# === Markdown Formatting ===
def format_chunk_as_markdown(chunk, metadata):
    md = '---\n'
    md += '\n'.join([f'{k}: {v}' for k, v in metadata.items()])
    md += '\n---\n\n'
    md += chunk
    return md

# === Streamlit UI ===
st.title("RAG Document Preparation Toolkit (All-in-One)")

st.markdown("""
Upload your documents (TXT, PDF, DOCX, PPTX), select chunk size, and download processed Markdown files with rich metadata. Optionally, enable LLM-powered enrichment (Perplexity API).
""")

chunk_size = st.number_input("Chunk size (words per chunk)", min_value=50, max_value=2000, value=500, step=50)
llm_enabled = st.checkbox("Enable LLM-powered metadata (Perplexity)", value=False)
PPLX_API_KEY = st.text_input("Perplexity API Key", value="", type="password")

if 'upload_temp_dir' not in st.session_state:
    st.session_state['upload_temp_dir'] = tempfile.mkdtemp(prefix="rag_upload_")
upload_temp_dir = Path(st.session_state['upload_temp_dir'])

uploaded_files = st.file_uploader("Upload files", type=["txt", "pdf", "docx", "pptx"], accept_multiple_files=True)

if uploaded_files:
    for uploaded_file in uploaded_files:
        file_path = upload_temp_dir / uploaded_file.name
        file_path.write_bytes(uploaded_file.read())

files_to_process = list(upload_temp_dir.glob("*.txt")) + list(upload_temp_dir.glob("*.pdf")) + \
                   list(upload_temp_dir.glob("*.docx")) + list(upload_temp_dir.glob("*.pptx"))

if st.button("Process"):
    if not files_to_process:
        st.warning("No files to process. Upload files first.")
    else:
        output_files = []
        llm_connector = LLMConnector(api_key=PPLX_API_KEY, model="sonar") if llm_enabled and PPLX_API_KEY else None
        for file_path in files_to_process:
            text = read_file_with_dispatcher(file_path)
            if not text:
                st.warning(f"Could not read {file_path.name}.")
                continue
            chunks = chunk_text_by_words(text, chunk_size)
            total_chunks = len(chunks)
            for idx, chunk in enumerate(chunks):
                metadata = extract_rich_metadata(file_path, chunk, idx+1, total_chunks, text)
                if llm_connector:
                    st.info(f"LLM prompt length: {len(chunk)} characters (pre-clean)")
                    llm_meta = llm_connector.enrich_metadata(chunk, text)
                    metadata.update(llm_meta)
                md_content = format_chunk_as_markdown(chunk, metadata)
                out_name = f"{file_path.stem}_chunk{idx+1}.txt"
                output_files.append((out_name, md_content))
        st.success(f"Processed {len(output_files)} chunks.")
        for out_name, md_content in output_files:
            st.download_button(
                label=f"Download {out_name}",
                data=md_content,
                file_name=out_name,
                mime="text/plain"
            ) 